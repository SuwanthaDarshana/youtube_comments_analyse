import re
import string
import torch
import joblib
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from youtube_comment_downloader import YoutubeCommentDownloader
from googletrans import Translator

# ---- Helper: Extract YouTube Video ID ----
def extract_video_id(url):
    match = re.search(r"(?:v=|\/)([0-9A-Za-z_-]{11}).*", url)
    return match.group(1) if match else None

# ---- Comment Extraction ----
def get_youtube_comments(video_url, max_comments=100):
    try:
        video_id = extract_video_id(video_url)
        if not video_id:
            raise ValueError("Invalid YouTube URL")
        downloader = YoutubeCommentDownloader()
        comments = []
        for comment in downloader.get_comments_from_url(f"https://www.youtube.com/watch?v={video_id}"):
            comments.append(comment['text'])
            if len(comments) >= max_comments:
                break
        return comments
    except Exception as e:
        print("Error while fetching comments:", e)
        return []

# ---- Translation ----
translator = Translator()
def is_sinhala(text):
    return bool(re.search(r'[\u0D80-\u0DFF]', text))

def translate_to_sinhala_if_needed(comments):
    translated_comments = []
    for comment in comments:
        comment = comment.strip()
        if not comment:
            continue
        if is_sinhala(comment):
            translated_comments.append(comment)
        else:
            try:
                translation = translator.translate(comment, src='auto', dest='si')
                translated_comments.append(translation.text.strip())
            except Exception as e:
                print(f"Translation failed: {e}")
                translated_comments.append(comment)
    return translated_comments

# ---- Preprocessing ----
sinhala_stopwords = set("""‡∑É‡∑Ñ ‡∑É‡∂∏‡∂ú ‡∑É‡∂∏‡∂ü ‡∂Ö‡∑Ñ‡∑è ‡∂Ü‡∑Ñ‡∑ä ‡∂Ü ‡∂ï‡∑Ñ‡∑ù ‡∂Ö‡∂±‡∑ö ‡∂Ö‡∂≥‡∑ù ‡∂Ö‡∂¥‡∑ú‡∂∫‡∑í ‡∂Ö‡∂¥‡∑ù ‡∂Ö‡∂∫‡∑í‡∂∫‡∑ù ‡∂Ü‡∂∫‡∑í ‡∂å‡∂∫‡∑í ‡∂†‡∑ì ‡∂†‡∑í‡∑Ñ‡∑ä ‡∂†‡∑í‡∂ö‡∑ä ‡∑Ñ‡∑ù‚Äç ‡∂Ø‡∑ù ‡∂Ø‡∑ù‡∑Ñ‡∑ù ‡∂∏‡∑ô‡∂±‡∑ä
‡∑É‡∑ö ‡∑Ä‡∑ê‡∂±‡∑í ‡∂∂‡∂≥‡∑î ‡∑Ä‡∂±‡∑ä ‡∂Ö‡∂∫‡∑î‡∂ª‡∑î ‡∂Ö‡∂∫‡∑î‡∂ª‡∑í‡∂±‡∑ä ‡∂Ω‡∑ô‡∑É ‡∑Ä‡∑ê‡∂©‡∑í ‡∑Å‡∑ä‚Äç‡∂ª‡∑ì ‡∑Ñ‡∑è ‡∂∫ ‡∂±‡∑í‡∑É‡∑è ‡∂±‡∑í‡∑É‡∑è‡∑Ä‡∑ô‡∂±‡∑ä ‡∂∂‡∑Ä‡∂ß ‡∂∂‡∑Ä ‡∂∂‡∑Ä‡∑ô‡∂±‡∑ä ‡∂±‡∂∏‡∑ä ‡∑Ä‡∑ê‡∂©‡∑í ‡∑É‡∑í‡∂ß‡∂Ø‡∑ì ‡∂∏‡∑Ñ‡∑è ‡∂∏‡∑Ñ
‡∂¥‡∂∏‡∂´ ‡∂¥‡∂∏‡∂´‡∑í‡∂±‡∑ä ‡∂¥‡∂∏‡∂± ‡∑Ä‡∂± ‡∑Ä‡∑í‡∂ß ‡∑Ä‡∑í‡∂ß‡∑í‡∂±‡∑ä ‡∂∏‡∑ö ‡∂∏‡∑ô‡∂Ω‡∑ô‡∑É ‡∂∏‡∑ô‡∂∫‡∑í‡∂±‡∑ä ‡∂á‡∂≠‡∑í ‡∂Ω‡∑ô‡∑É ‡∑É‡∑í‡∂Ø‡∑î ‡∑Ä‡∑Å‡∂∫‡∑ô‡∂±‡∑ä ‡∂∫‡∂± ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂∏‡∂ú‡∑í‡∂±‡∑ä ‡∑Ñ‡∑ù‚Äç ‡∂â‡∂≠‡∑è ‡∂í ‡∂ë‡∂∏ ‡∂Ø
‡∂Ö‡∂≠‡∂ª ‡∑Ä‡∑í‡∑É‡∑í‡∂±‡∑ä ‡∑É‡∂∏‡∂ú ‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥‡∑Ä ‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥ ‡∂≠‡∑î‡∑Ö ‡∂∂‡∑Ä ‡∑Ä‡∑ê‡∂±‡∑í ‡∂∏‡∑Ñ ‡∂∏‡∑ô‡∂∏ ‡∂∏‡∑ô‡∑Ñ‡∑í ‡∂∏‡∑ö ‡∑Ä‡∑ô‡∂≠ ‡∑Ä‡∑ô‡∂≠‡∑í‡∂±‡∑ä ‡∑Ä‡∑ô‡∂≠‡∂ß ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∑ô‡∂±‡∑ä ‡∑Ä‡∑ô‡∂±‡∑î‡∑Ä‡∂ß
‡∑Ä‡∑ô‡∂± ‡∂ú‡∑ê‡∂± ‡∂±‡∑ë ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂±‡∑Ä ‡∂¥‡∑í‡∑Ö‡∑í‡∂∂‡∂≥ ‡∑Ä‡∑í‡∑Å‡∑ö‡∑Ç ‡∂Ø‡∑ê‡∂±‡∂ß ‡∂ë‡∑Ñ‡∑ô‡∂±‡∑ä ‡∂∏‡∑ô‡∑Ñ‡∑ô‡∂±‡∑ä ‡∂ë‡∑Ñ‡∑ö ‡∂∏‡∑ô‡∑Ñ‡∑ö ‡∂∏ ‡∂≠‡∑Ä‡∂≠‡∑ä ‡∂≠‡∑Ä ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è ‡∂ß ‡∂ú‡∑ö ‡∂ë ‡∂ö ‡∂ö‡∑ä
‡∂∂‡∑Ä‡∂≠‡∑ä ‡∂∂‡∑Ä‡∂Ø ‡∂∏‡∂≠ ‡∂á‡∂≠‡∑î‡∂Ω‡∑î ‡∂á‡∂≠‡∑î‡∑Ö‡∑î ‡∂∏‡∑ô‡∑É‡∑ö ‡∑Ä‡∑ê‡∂©‡∑í ‡∑Ä‡∂©‡∑è ‡∑Ä‡∂©‡∑è‡∂≠‡∑ä‡∂∏ ‡∂±‡∑í‡∂≠‡∑í ‡∂±‡∑í‡∂≠‡∑í‡∂≠‡∑ä ‡∂±‡∑í‡∂≠‡∑ú‡∂ª ‡∂±‡∑í‡∂≠‡∂ª ‡∂â‡∂ö‡∑ä‡∂∂‡∑í‡∂≠‡∑í ‡∂Ø‡∑ê‡∂±‡∑ä ‡∂∫‡∂Ω‡∑í ‡∂¥‡∑î‡∂± ‡∂â‡∂≠‡∑í‡∂±‡∑ä ‡∑É‡∑í‡∂ß
‡∑É‡∑í‡∂ß‡∂±‡∑ä ‡∂¥‡∂ß‡∂±‡∑ä ‡∂≠‡∑ô‡∂ö‡∑ä ‡∂Ø‡∂ö‡∑ä‡∑Ä‡∑è ‡∑É‡∑è ‡∂≠‡∑è‡∂ö‡∑ä ‡∂≠‡∑î‡∑Ä‡∂ö‡∑ä ‡∂¥‡∑Ä‡∑è ‡∂Ø ‡∑Ñ‡∑ù‚Äç ‡∑Ä‡∂≠‡∑ä ‡∑Ä‡∑í‡∂±‡∑è ‡∑Ñ‡∑ê‡∂ª ‡∂∏‡∑í‡∑É ‡∂∏‡∑î‡∂≠‡∑ä ‡∂ö‡∑í‡∂∏ ‡∂ö‡∑í‡∂∏‡∑ä ‡∂á‡∂∫‡∑í ‡∂∏‡∂±‡∑ä‡∂Ø ‡∑Ñ‡∑ô‡∑Ä‡∂≠‡∑ä ‡∂±‡∑ú‡∑Ñ‡∑ú‡∂≠‡∑ä
‡∂¥‡∂≠‡∑è ‡∂¥‡∑è‡∑É‡∑è ‡∂ú‡∑è‡∂±‡∑ô ‡∂≠‡∑Ä ‡∂â‡∂≠‡∑è ‡∂∂‡∑ú‡∑Ñ‡∑ù ‡∑Ä‡∑Ñ‡∑è ‡∑É‡∑ô‡∂Ø ‡∑É‡∑ê‡∂±‡∑í‡∂±‡∑ä ‡∑Ñ‡∂±‡∑í‡∂ö ‡∂ë‡∂∏‡∑ä‡∂∂‡∑è ‡∂ë‡∂∏‡∑ä‡∂∂‡∂Ω ‡∂∂‡∑ú‡∂Ω ‡∂±‡∂∏‡∑ä ‡∑Ä‡∂±‡∑è‡∑Ñ‡∑í ‡∂ö‡∂Ω‡∑ì ‡∂â‡∂≥‡∑î‡∂ª‡∑è ‡∂Ö‡∂±‡∑ä‡∂± ‡∂î‡∂±‡∑ä‡∂±
‡∂∏‡∑ô‡∂±‡∑ä‡∂± ‡∂ã‡∂Ø‡∑ô‡∑É‡∑è ‡∂¥‡∑í‡∂´‡∑í‡∑É ‡∑É‡∂≥‡∑Ñ‡∑è ‡∂Ö‡∂ª‡∂∂‡∂∫‡∑è ‡∂±‡∑í‡∑É‡∑è ‡∂ë‡∂±‡∑í‡∑É‡∑è ‡∂ë‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä ‡∂∂‡∑ê‡∑Ä‡∑í‡∂±‡∑ä ‡∑Ñ‡∑ô‡∂∫‡∑í‡∂±‡∑ä ‡∑É‡∑ö‡∂ö‡∑ä ‡∑É‡∑ö‡∂ö ‡∂ú‡∑ê‡∂± ‡∂Ö‡∂±‡∑î‡∑Ä ‡∂¥‡∂ª‡∑í‡∂Ø‡∑í ‡∑Ä‡∑í‡∂ß ‡∂≠‡∑ô‡∂ö‡∑ä
‡∂∏‡∑ô‡∂≠‡∑ô‡∂ö‡∑ä ‡∂∏‡∑ö‡∂≠‡∑è‡∂ö‡∑ä ‡∂≠‡∑î‡∂ª‡∑î ‡∂≠‡∑î‡∂ª‡∑è ‡∂≠‡∑î‡∂ª‡∑è‡∑Ä‡∂ß ‡∂≠‡∑î‡∂Ω‡∑í‡∂±‡∑ä ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∂ë‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∑Ä‡∑É‡∑ä ‡∂∏‡∑ô‡∂±‡∑ä ‡∂Ω‡∑ô‡∑É ‡∂¥‡∂ª‡∑í‡∂Ø‡∑í ‡∂ë‡∑Ñ‡∑ô‡∂≠‡∑ä""".split())  

def remove_links(text): return re.sub(r'https?://\S+|www\.\S+', '', text)
def remove_english(text): return re.sub(r'[a-zA-Z]', '', text)
def remove_punctuations(text): return text.translate(str.maketrans('', '', string.punctuation))
def remove_numbers(text): return re.sub(r'\d+', '', text)
def remove_non_sinhala(text): return re.sub(r'[^\u0D80-\u0DFF\s]', '', str(text))
def remove_emojis(text):
    emoji_pattern = re.compile("[" +
        u"\U0001F600-\U0001F64F" +
        u"\U0001F300-\U0001F5FF" +
        u"\U0001F680-\U0001F6FF" +
        u"\U0001F1E0-\U0001F1FF" +
        u"\U00002700-\U000027BF" +
        u"\U000024C2-\U0001F251" +
        "]", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_sinhala_stopwords(text):
    return " ".join([word for word in text.split() if word not in sinhala_stopwords])

def full_clean(text):
    text = text.strip()
    text = remove_links(text)
    text = remove_emojis(text)
    text = remove_english(text)
    text = remove_punctuations(text)
    text = remove_numbers(text)
    text = remove_non_sinhala(text)
    text = remove_sinhala_stopwords(text)
    return text.strip()

def preprocess_comments(comments):
    return [full_clean(c) for c in comments if c.strip()]

# ---- Tokenizer ----
tokenizer = AutoTokenizer.from_pretrained("./notebooks/saved_model", local_files_only=True)
def tokenize_combined_text(text):
    return tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors="pt")

# ---- Load Model and LabelEncoder ----
model = AutoModelForSequenceClassification.from_pretrained("./notebooks/saved_model", local_files_only=True)
label_encoder = joblib.load("./notebooks/saved_model/label_encoder.joblib")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# ---- Main Function ----
def predict_youtube_category(video_url):
    print("\nüîç Fetching comments...")
    comments = get_youtube_comments(video_url)
    if not comments:
        print("‚ùå No comments fetched.")
        return None

    print(f"üó£ Translating and cleaning {len(comments)} comments...")
    comments = translate_to_sinhala_if_needed(comments)
    cleaned_comments = preprocess_comments(comments)

    print("\nüßπ Preprocessed Comments Sample:")
    for i, comment in enumerate(cleaned_comments[:100], start=1):
        print(f"{i}. {comment}")

    if not cleaned_comments:
        print("‚ùå All comments were empty after cleaning.")
        return None

    combined_text = " ".join(cleaned_comments)
    inputs = tokenize_combined_text(combined_text)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    print("ü§ñ Predicting category...")
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class_id = torch.argmax(logits, dim=1).item()
        predicted_label = label_encoder.inverse_transform([predicted_class_id])[0]

    print(f"‚úÖ Predicted Category: **{predicted_label}**")
    return predicted_label
